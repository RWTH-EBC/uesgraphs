{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import uesgraphs as ug\n",
    "\n",
    "from uesgraphs.examples import e1_example_readme as e1\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uesgraphs.analysis as analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "workspace = e1.workspace_example(\"e14\")\n",
    "\n",
    "dir_ues = os.path.dirname(os.path.dirname(workspace))\n",
    "pinola_json = os.path.join(dir_ues, \"workspace\", \"e11\", \"inputs\",\"test_modelgen\", \"Pinola\", \"nodes.json\")\n",
    "pinola_sim_data = os.path.join(dir_ues,\"uesgraphs\",\"data\",\"Pinola_low_temp_network_inputs.mat\")\n",
    "pinola_sim_data = r\"E:\\rka_lko\\work\\2025_04_analysis\\10042025SeestadtNewSim\\Sim20250409_190504_detailed\\Sim20250409_190504_1\\Results\\Sim20250409_190504_1_inputs.gzip\"\n",
    "pinola_json =r\"E:\\rka_lko\\git\\transurban_seestadt\\dhc_model\\workspace\\transurban_seestadt_uesgraphs.json\"\n",
    "sysm_model = r\"E:\\rka_lko\\git\\transurban_seestadt\\dhc_model\\workspace\\model.json\"\n",
    "if not os.path.exists(pinola_json):\n",
    "    raise FileNotFoundError(f\"File {pinola_json} not found.\"\n",
    "                            \"Please run example e11 to generate network topology.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read nodes...\n",
      "******\n",
      " input_ids were {'buildings': None, 'nodes': 'f607cb63-8aed-446c-9970-9cd8380434f8', 'pipes': None, 'supplies': None}\n",
      "...finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "graph = ug.UESGraph()\n",
    "graph.from_json(path = pinola_json, network_type=\"heating\")\n",
    "graph.graph[\"name\"] = \"pinola\"\n",
    "graph.graph[\"supply_type\"] = \"supply\"\n",
    "\n",
    "start_date=datetime(2024, 1, 1) \n",
    "end_date=datetime(2024, 1, 7)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logfile findable here: C:\\Users\\rka-lko\\AppData\\Local\\Temp\\uesgraphs.analysis.data_handling.data_handling.assign_data_to_uesgraphs_20250605_181236.log\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_node_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m gr = \u001b[43manalysis\u001b[49m\u001b[43m.\u001b[49m\u001b[43massign_simulation_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpinola_sim_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43maixlib_version\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2.0.0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mauto_retry\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\rka-lko\\git\\uesgraphs\\uesgraphs\\analysis\\data_handling\\data_handling.py:554\u001b[39m, in \u001b[36massign_simulation_data\u001b[39m\u001b[34m(graph, sim_data, start_date, end_date, aixlib_version, time_interval, auto_retry, validate_network, log_dir)\u001b[39m\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m graph_with_nodes\n\u001b[32m    553\u001b[39m \u001b[38;5;66;03m# First attempt with specified version\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m result = \u001b[43m_attempt_assignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43maixlib_version\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    555\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m    556\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mUESGraph data assignment completed successfully\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\rka-lko\\git\\uesgraphs\\uesgraphs\\analysis\\data_handling\\data_handling.py:518\u001b[39m, in \u001b[36massign_simulation_data.<locals>._attempt_assignment\u001b[39m\u001b[34m(version)\u001b[39m\n\u001b[32m    516\u001b[39m \u001b[38;5;66;03m# Assign node values (pressure and temperature)\u001b[39;00m\n\u001b[32m    517\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mAssigning node values (pressure and temperature)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m518\u001b[39m graph_with_nodes = \u001b[43mget_node_values\u001b[49m(graph, df.iloc[\u001b[32m0\u001b[39m], \n\u001b[32m    519\u001b[39m                                  pipe_type=supply_type_prefix[supply_type],\n\u001b[32m    520\u001b[39m                                  logger=logger)\n\u001b[32m    522\u001b[39m \u001b[38;5;66;03m# Assign time series data to nodes\u001b[39;00m\n\u001b[32m    523\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mAssigning time series data to nodes\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'get_node_values' is not defined"
     ]
    }
   ],
   "source": [
    "gr = analysis.assign_simulation_data(graph, pinola_sim_data, start_date, end_date,aixlib_version=\"2.0.0\",auto_retry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logfile findable here: C:\\Users\\rka-lko\\AppData\\Local\\Temp\\3\\SystemModelHeating_20250821_152127.log\n",
      "Warning: Could not set attribute time: property 'time' of 'SystemModelHeating' object has no setter\n",
      "Model loaded from E:\\rka_lko\\git\\transurban_seestadt\\dhc_model\\workspace\\model.json\n"
     ]
    }
   ],
   "source": [
    "from uesgraphs.systemmodels import utilities as ut\n",
    "sysm_graph = ut.load_system_model_from_json(sysm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uesgraphs.analysis.data_handling import graph_transformation\n",
    "port_mapping = graph_transformation.map_system_model_to_uesgraph(sysm_graph,graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "uesgraphs.analysis.data_handling.data_handling.get_MASKS - INFO - Using AixLib version 2.1.0 masks\n",
      "uesgraphs.analysis.data_handling.data_handling.process_parquet_file - INFO - Found existing gzip file: E:\\rka_lko\\work\\2025_04_analysis\\10042025SeestadtNewSim\\Sim20250409_190504_detailed\\Sim20250409_190504_1\\Results\\Sim20250409_190504_1_inputs.gzip\n",
      "uesgraphs.analysis.data_handling.data_handling.process_parquet_file - INFO - Validating 72 required columns in: E:\\rka_lko\\work\\2025_04_analysis\\10042025SeestadtNewSim\\Sim20250409_190504_detailed\\Sim20250409_190504_1\\Results\\Sim20250409_190504_1_inputs.gzip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter-Liste mit 72 Variablen:\n",
      "  - networkModel.pipe10021010.port_a.m_flow\n",
      "  - networkModel.pipe10021010.dp\n",
      "  - networkModel.pipe10021010.port_a.p\n",
      "  - networkModel.pipe10021010.port_b.p\n",
      "  - networkModel.pipe10021010.sta_a.T\n",
      "  - networkModel.pipe10021010.sta_b.T\n",
      "  - networkModel.pipe10021014.port_a.m_flow\n",
      "  - networkModel.pipe10021014.dp\n",
      "  - networkModel.pipe10021014.port_a.p\n",
      "  - networkModel.pipe10021014.port_b.p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "uesgraphs.analysis.data_handling.data_handling.process_parquet_file - INFO - ✅ All required columns found in data file\n",
      "uesgraphs.analysis.data_handling.data_handling.process_parquet_file - INFO - Starting parquet file processing: E:\\rka_lko\\work\\2025_04_analysis\\10042025SeestadtNewSim\\Sim20250409_190504_detailed\\Sim20250409_190504_1\\Results\\Sim20250409_190504_1_inputs.gzip\n",
      "uesgraphs.analysis.data_handling.data_handling.process_parquet_file - INFO - Successfully loaded 35041 rows, 72 columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Shape: (35041, 72)\n",
      "Spalten: ['networkModel.pipe10021010.port_a.m_flow', 'networkModel.pipe10021010.dp', 'networkModel.pipe10021010.port_a.p', 'networkModel.pipe10021010.port_b.p', 'networkModel.pipe10021010.sta_a.T', 'networkModel.pipe10021010.sta_b.T', 'networkModel.pipe10021014.port_a.m_flow', 'networkModel.pipe10021014.dp', 'networkModel.pipe10021014.port_a.p', 'networkModel.pipe10021014.port_b.p', 'networkModel.pipe10021014.sta_a.T', 'networkModel.pipe10021014.sta_b.T', 'networkModel.pipe10021025.port_a.m_flow', 'networkModel.pipe10021025.dp', 'networkModel.pipe10021025.port_a.p', 'networkModel.pipe10021025.port_b.p', 'networkModel.pipe10021025.sta_a.T', 'networkModel.pipe10021025.sta_b.T', 'networkModel.pipe10041006.port_a.m_flow', 'networkModel.pipe10041006.dp', 'networkModel.pipe10041006.port_a.p', 'networkModel.pipe10041006.port_b.p', 'networkModel.pipe10041006.sta_a.T', 'networkModel.pipe10041006.sta_b.T', 'networkModel.pipe10041007.port_a.m_flow', 'networkModel.pipe10041007.dp', 'networkModel.pipe10041007.port_a.p', 'networkModel.pipe10041007.port_b.p', 'networkModel.pipe10041007.sta_a.T', 'networkModel.pipe10041007.sta_b.T', 'networkModel.pipe10041027.port_a.m_flow', 'networkModel.pipe10041027.dp', 'networkModel.pipe10041027.port_a.p', 'networkModel.pipe10041027.port_b.p', 'networkModel.pipe10041027.sta_a.T', 'networkModel.pipe10041027.sta_b.T', 'networkModel.pipe10061022.port_a.m_flow', 'networkModel.pipe10061022.dp', 'networkModel.pipe10061022.port_a.p', 'networkModel.pipe10061022.port_b.p', 'networkModel.pipe10061022.sta_a.T', 'networkModel.pipe10061022.sta_b.T', 'networkModel.pipe10071018.port_a.m_flow', 'networkModel.pipe10071018.dp', 'networkModel.pipe10071018.port_a.p', 'networkModel.pipe10071018.port_b.p', 'networkModel.pipe10071018.sta_a.T', 'networkModel.pipe10071018.sta_b.T', 'networkModel.pipe10101029.port_a.m_flow', 'networkModel.pipe10101029.dp', 'networkModel.pipe10101029.port_a.p', 'networkModel.pipe10101029.port_b.p', 'networkModel.pipe10101029.sta_a.T', 'networkModel.pipe10101029.sta_b.T', 'networkModel.pipe10141022.port_a.m_flow', 'networkModel.pipe10141022.dp', 'networkModel.pipe10141022.port_a.p', 'networkModel.pipe10141022.port_b.p', 'networkModel.pipe10141022.sta_a.T', 'networkModel.pipe10141022.sta_b.T', 'networkModel.pipe10141026.port_a.m_flow', 'networkModel.pipe10141026.dp', 'networkModel.pipe10141026.port_a.p', 'networkModel.pipe10141026.port_b.p', 'networkModel.pipe10141026.sta_a.T', 'networkModel.pipe10141026.sta_b.T', 'networkModel.pipe10181028.port_a.m_flow', 'networkModel.pipe10181028.dp', 'networkModel.pipe10181028.port_a.p', 'networkModel.pipe10181028.port_b.p', 'networkModel.pipe10181028.sta_a.T', 'networkModel.pipe10181028.sta_b.T']\n"
     ]
    }
   ],
   "source": [
    "# 1. Supply Type und Masks laden\n",
    "supply_type = graph.graph[\"supply_type\"]  # \"supply\" oder \"return\"\n",
    "supply_type_prefix = {\"supply\": \"\", \"return\": \"R\"}\n",
    "\n",
    "from uesgraphs.analysis.data_handling.data_handling import get_MASKS\n",
    "masks = get_MASKS(\"2.1.0\")  # oder deine gewünschte Version\n",
    "\n",
    "# 2. Filter-Liste bauen (wie im existierenden Code)\n",
    "filter_list = []\n",
    "for edge in graph.edges:\n",
    "    pipe_code = graph.edges[edge][\"name\"]\n",
    "    for mask_type, mask_pattern in masks.items():\n",
    "        var_name = mask_pattern.format(\n",
    "            pipe_code=pipe_code, \n",
    "            type=supply_type_prefix[supply_type]\n",
    "        )\n",
    "        filter_list.append(var_name)\n",
    "\n",
    "print(f\"Filter-Liste mit {len(filter_list)} Variablen:\")\n",
    "for var in filter_list[:10]:  # Erste 10 zeigen\n",
    "    print(f\"  - {var}\")\n",
    "\n",
    "# 3. Daten laden\n",
    "from uesgraphs.analysis.data_handling.data_handling import process_simulation_result\n",
    "df = process_simulation_result(file_path=pinola_sim_data, filter_list=filter_list)\n",
    "\n",
    "print(f\"\\nDataFrame Shape: {df.shape}\")\n",
    "print(f\"Spalten: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'m_flow': 'networkModel.pipe{pipe_code}{type}.port_a.m_flow',\n",
       " 'dp': 'networkModel.pipe{pipe_code}{type}.dp',\n",
       " 'p_a': 'networkModel.pipe{pipe_code}{type}.port_a.p',\n",
       " 'p_b': 'networkModel.pipe{pipe_code}{type}.port_b.p',\n",
       " 'T_a': 'networkModel.pipe{pipe_code}{type}.sta_a.T',\n",
       " 'T_b': 'networkModel.pipe{pipe_code}{type}.sta_b.T'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pipe10021014.port_a'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1001\n",
    "node_ports = port_mapping[i]  # Liste aller Ports für diesen Node\n",
    "collected_values = {\"T\": [], \"p\": []}\n",
    "\n",
    "# Mapping für flexible Attributnamen im Graph\n",
    "attribute_names = {\"T\": \"temperature\", \"p\": \"press_flow\"}  # Konfigurierbar!\n",
    "\n",
    "for port in node_ports:  # Iteriere über ALLE Ports des Nodes\n",
    "    port_splitted = port.split(\".\")\n",
    "    if port_splitted[1].endswith(\"a\"):\n",
    "        #port is an a port\n",
    "        temp = f\"networkModel.{port_splitted[0]}.sta_a.T\"\n",
    "        press = f\"networkModel.{port_splitted[0]}.port_a.p\"\n",
    "        t_value = df[temp].iloc[0]\n",
    "        p_value = df[press].iloc[0]\n",
    "        collected_values[\"T\"].append(t_value)\n",
    "        collected_values[\"p\"].append(p_value)\n",
    "\n",
    "# Validiere und weise zu\n",
    "for size in [\"T\", \"p\"]:\n",
    "    unique_vals = set(collected_values[size])\n",
    "    if len(unique_vals) != 1:\n",
    "        raise ValueError(f\"Node {i}: Inkonsistente {size}-Werte gefunden: {collected_values[size]}\")\n",
    "    graph.nodes[i][attribute_names[size]] = collected_values[size][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Port Mapping für Knoten {i}: {port}\")\n",
    "pipe_mask = df.columns.str.contains('pipe10021014.')\n",
    "df_pipe = df.loc[:, pipe_mask]\n",
    "print(f\"Gefundene Spalten: {df.columns[pipe_mask].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge (1001, 1005) has data: {'diameter': 0.0697, 'length': 62.67448664094058, 'pipeID': '10021010', 'name': '10021010', 'node_0': 1002, 'node_1': 1010}\n",
      "Edge (1001, 1006) has data: {'diameter': 0.2101, 'length': 40.10060652984215, 'pipeID': '10021014', 'name': '10021014', 'node_0': 1002, 'node_1': 1014}\n",
      "Edge (1001, 1009) has data: {'diameter': 0.2101, 'length': 23.793988913832845, 'pipeID': '10021025', 'name': '10021025', 'node_0': 1002, 'node_1': 'supply1'}\n",
      "Edge (1002, 1003) has data: {'diameter': 0.1325, 'length': 23.15227945754485, 'pipeID': '10041006', 'name': '10041006', 'node_0': 1004, 'node_1': 1006}\n",
      "Edge (1002, 1004) has data: {'diameter': 0.1325, 'length': 67.21343661070965, 'pipeID': '10041007', 'name': '10041007', 'node_0': 1004, 'node_1': 1007}\n",
      "Edge (1002, 1011) has data: {'diameter': 0.0539, 'length': 25.20026053450003, 'pipeID': '10041027', 'name': '10041027', 'node_0': 1004, 'node_1': 'w11_2'}\n",
      "Edge (1003, 1008) has data: {'diameter': 0.2101, 'length': 43.10762795752977, 'pipeID': '10061022', 'name': '10061022', 'node_0': 1006, 'node_1': 1022}\n",
      "Edge (1004, 1007) has data: {'diameter': 0.1071, 'length': 41.06543024277208, 'pipeID': '10071018', 'name': '10071018', 'node_0': 1007, 'node_1': 1018}\n",
      "Edge (1005, 1013) has data: {'diameter': 0.0697, 'length': 17.635612191884555, 'pipeID': '10101029', 'name': '10101029', 'node_0': 1010, 'node_1': 'w11_4'}\n",
      "Edge (1006, 1008) has data: {'diameter': 0.2101, 'length': 42.097964605664814, 'pipeID': '10141022', 'name': '10141022', 'node_0': 1014, 'node_1': 1022}\n",
      "Edge (1006, 1010) has data: {'diameter': 0.0539, 'length': 18.20446528640188, 'pipeID': '10141026', 'name': '10141026', 'node_0': 1014, 'node_1': 'w11_3'}\n",
      "Edge (1007, 1012) has data: {'diameter': 0.0697, 'length': 22.026936306526547, 'pipeID': '10181028', 'name': '10181028', 'node_0': 1018, 'node_1': 'w11_1'}\n"
     ]
    }
   ],
   "source": [
    "edges = list(graph.edges)\n",
    "for edge in edges:\n",
    "    print(f\"Edge {edge} has data: {graph.edges[edge]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    graph = analyze.assign_data_to_uesgraphs(graph,sim_data = pinola_sim_data,\n",
    "                                             start_date=start_date,\n",
    "                                             end_date=end_date,\n",
    "                                             aixlib_version=\"2.1.0\") #aixlib version is needed to assign data properly\n",
    "    \n",
    "    ### Plotting\n",
    "    #Visuals cant handle series data, so we just take the mean values, but single time points are possible\n",
    "    vis = ug.Visuals(graph)\n",
    "    for edge in graph.edges:\n",
    "        graph.edges[edge][\"m_flow_mean\"] = graph.edges[edge][\"m_flow\"].mean()\n",
    "    vis.show_network(show_plot=False,\n",
    "                           scaling_factor=1,\n",
    "                           scaling_factor_diameter=50,\n",
    "                           label_size=15,\n",
    "                           ylabel=\"Mean mass flow [kg/s]\",\n",
    "                           generic_extensive_size=\"m_flow_mean\",\n",
    "                           save_as=os.path.join(workspace, \"m_flow.png\"),\n",
    "                           timestamp=f\"{graph.graph[\"name\"]}: Mean mass flow\"\n",
    "                           )\n",
    "    \n",
    "    for node in graph.nodes:\n",
    "        graph.nodes[node][\"press_flow_mean\"] = graph.nodes[node][\"press_flow\"].mean()\n",
    "    vis.show_network(show_plot=False,\n",
    "                           scaling_factor=1,\n",
    "                           scaling_factor_diameter=50,\n",
    "                           ylabel=\"Mean pressure [Pa]\",\n",
    "                           label_size=15,\n",
    "                           generic_intensive_size=\"press_flow_mean\",\n",
    "                           save_as=os.path.join(workspace, \"press_flow.png\"),\n",
    "                           timestamp=f\"{graph.graph[\"name\"]}: Mean pressure flow\"\n",
    "                           )\n",
    "    \n",
    "    df = analyze.pump_power_analysis(graph, True, r\"D:\\rka-lko\\work\\2025_04_analysis\\3\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "source": "def get_aixlib_masks_for_nodes(aixlib_version=\"2.1.0\"):\n    \"\"\"\n    Get node-specific attribute patterns based on AixLib version.\n    \n    Different AixLib versions use different naming conventions:\n    - 2.1.0: port_b.p, sta_b.T  \n    - 2.0.0: ports_b[1].p, sta_b[1].T\n    \n    Returns:\n        dict: Node attribute patterns for flexible assignment\n    \"\"\"\n    from uesgraphs.analysis.data_handling.data_handling import get_MASKS\n    \n    masks = get_MASKS(aixlib_version)\n    \n    # Extract node patterns from edge masks\n    # Example: \"networkModel.pipe{pipe_code}{type}.port_a.p\" -> \"port_{port}.p\"\n    node_patterns = {}\n    \n    for attr, mask in masks.items():\n        if attr in [\"T_a\", \"T_b\"]:\n            # Temperature patterns\n            if \"temperature\" not in node_patterns:\n                node_patterns[\"temperature\"] = []\n            suffix = mask.split(\".\")[-1]  # Get \"T\"\n            port_pattern = mask.split(\".\")[-2]  # Get \"sta_a\" or \"sta_b[1]\"\n            node_patterns[\"temperature\"].append(f\"{port_pattern}.{suffix}\")\n            \n        elif attr in [\"p_a\", \"p_b\"]:\n            # Pressure patterns  \n            if \"pressure\" not in node_patterns:\n                node_patterns[\"pressure\"] = []\n            suffix = mask.split(\".\")[-1]  # Get \"p\"\n            port_pattern = mask.split(\".\")[-2]  # Get \"port_a\" or \"ports_b[1]\"\n            node_patterns[\"pressure\"].append(f\"{port_pattern}.{suffix}\")\n    \n    # Convert to format our flexible function expects\n    final_patterns = {}\n    for attr, patterns in node_patterns.items():\n        final_patterns[attr] = []\n        for pattern in patterns:\n            # Replace port identifiers with our placeholder\n            if \"port_a\" in pattern:\n                final_patterns[attr].append(pattern.replace(\"port_a\", \"port_{port}\"))\n            elif \"port_b\" in pattern or \"ports_b\" in pattern:\n                final_patterns[attr].append(pattern.replace(\"port_b\", \"port_{port}\").replace(\"ports_b[1]\", \"port_{port}\"))\n            elif \"sta_a\" in pattern:\n                final_patterns[attr].append(pattern.replace(\"sta_a\", \"sta_{port}\"))\n            elif \"sta_b\" in pattern:\n                final_patterns[attr].append(pattern.replace(\"sta_b\", \"sta_{port}\"))\n    \n    return final_patterns\n\ndef assign_edge_data_with_masks(graph, df, aixlib_version=\"2.1.0\", time_index=0, verbose=True):\n    \"\"\"\n    Assign simulation data to graph edges using AixLib version-aware masks.\n    \n    This function handles the complex naming differences between AixLib versions\n    and maps simulation variables to meaningful edge attributes.\n    \n    Args:\n        graph: UESGraph object\n        df: DataFrame with simulation data  \n        aixlib_version: AixLib version used in simulation\n        time_index: Time point to extract (0 = first timestep)\n        verbose: Print progress information\n        \n    Returns:\n        dict: Summary of edge assignment results\n    \"\"\"\n    from uesgraphs.analysis.data_handling.data_handling import get_MASKS, check_supply_type\n    \n    if verbose:\n        print(f\"Assigning edge data using AixLib {aixlib_version} masks...\")\n    \n    # Get version-specific masks and supply type\n    masks = get_MASKS(aixlib_version)\n    supply_type = check_supply_type(graph)\n    supply_type_prefix = {\"supply\": \"\", \"return\": \"R\"}\n    type_suffix = supply_type_prefix[supply_type]\n    \n    # Better attribute names for edges\n    attribute_mapping = {\n        \"m_flow\": \"mass_flow\",\n        \"dp\": \"pressure_drop\",\n        \"p_a\": \"pressure_inlet\", \n        \"p_b\": \"pressure_outlet\",\n        \"T_a\": \"temperature_inlet\",\n        \"T_b\": \"temperature_outlet\"\n    }\n    \n    results = {\"processed\": 0, \"skipped\": 0, \"missing_data\": {}, \"assigned_attributes\": []}\n    \n    for edge in graph.edges:\n        pipe_name = graph.edges[edge].get(\"name\", graph.edges[edge].get(\"pipeID\"))\n        if not pipe_name:\n            if verbose:\n                print(f\"  Skipping edge {edge}: no pipe name found\")\n            results[\"skipped\"] += 1\n            continue\n        \n        edge_data = {}\n        missing_vars = []\n        \n        # Try to assign each available attribute\n        for mask_key, mask_pattern in masks.items():\n            var_name = mask_pattern.format(pipe_code=pipe_name, type=type_suffix)\n            \n            if var_name in df.columns:\n                value = df[var_name].iloc[time_index] if isinstance(time_index, int) else df[var_name].iloc[time_index]\n                attr_name = attribute_mapping.get(mask_key, mask_key)\n                edge_data[attr_name] = float(value) if isinstance(time_index, int) else value\n                \n                if attr_name not in results[\"assigned_attributes\"]:\n                    results[\"assigned_attributes\"].append(attr_name)\n            else:\n                missing_vars.append(var_name)\n        \n        # Store results\n        if missing_vars:\n            results[\"missing_data\"][edge] = missing_vars\n        \n        # Assign to graph\n        for attr_name, value in edge_data.items():\n            graph.edges[edge][attr_name] = value\n        \n        results[\"processed\"] += 1\n        \n        if verbose and results[\"processed\"] <= 3:\n            print(f\"  Edge {edge}: assigned {list(edge_data.keys())}\")\n    \n    if verbose:\n        print(f\"  → Processed {results['processed']} edges\")\n        print(f\"  → Available attributes: {results['assigned_attributes']}\")\n        \n    return results\n\ndef calculate_network_statistics(graph):\n    \"\"\"\n    Calculate comprehensive network statistics after data assignment.\n    \n    Returns:\n        dict: Network statistics including coverage and value ranges\n    \"\"\"\n    stats = {\"nodes\": {}, \"edges\": {}, \"coverage\": {}}\n    \n    # Node statistics\n    node_attrs = [\"temperature\", \"pressure\"]\n    for attr in node_attrs:\n        values = [graph.nodes[n].get(attr, None) for n in graph.nodes]\n        valid_values = [v for v in values if v is not None]\n        \n        stats[\"nodes\"][attr] = {\n            \"count\": len(valid_values),\n            \"coverage\": len(valid_values) / len(graph.nodes) * 100,\n            \"min\": min(valid_values) if valid_values else None,\n            \"max\": max(valid_values) if valid_values else None,\n            \"mean\": sum(valid_values) / len(valid_values) if valid_values else None\n        }\n    \n    # Edge statistics  \n    edge_attrs = [\"mass_flow\", \"pressure_drop\", \"temperature_inlet\", \"temperature_outlet\"]\n    for attr in edge_attrs:\n        values = [graph.edges[e].get(attr, None) for e in graph.edges]\n        valid_values = [v for v in values if v is not None]\n        \n        stats[\"edges\"][attr] = {\n            \"count\": len(valid_values),\n            \"coverage\": len(valid_values) / len(graph.edges) * 100 if graph.edges else 0,\n            \"min\": min(valid_values) if valid_values else None,\n            \"max\": max(valid_values) if valid_values else None,\n            \"mean\": sum(valid_values) / len(valid_values) if valid_values else None\n        }\n    \n    # Print summary\n    print(\"Node coverage:\")\n    for attr, data in stats[\"nodes\"].items():\n        print(f\"  {attr}: {data['coverage']:.1f}% ({data['count']}/{len(graph.nodes)})\")\n        if data['min'] is not None:\n            print(f\"    Range: {data['min']:.2f} - {data['max']:.2f}\")\n    \n    print(\"Edge coverage:\")\n    for attr, data in stats[\"edges\"].items():\n        if data['count'] > 0:\n            print(f\"  {attr}: {data['coverage']:.1f}% ({data['count']}/{len(graph.edges)})\")\n            print(f\"    Range: {data['min']:.3f} - {data['max']:.3f}\")\n    \n    return stats\n\ndef full_network_assignment_modular(graph, df, port_mapping, aixlib_version=\"2.1.0\", time_index=0):\n    \"\"\"\n    Complete modular network data assignment.\n    \n    This function orchestrates the entire process:\n    1. Node value assignment with flexible patterns\n    2. Edge value assignment with AixLib masks  \n    3. Network validation and statistics\n    \n    Args:\n        graph: UESGraph object to populate\n        df: Simulation data DataFrame\n        port_mapping: Node to port mapping from system model\n        aixlib_version: AixLib version for correct variable naming\n        time_index: Time point to extract data from\n        \n    Returns:\n        dict: Complete assignment results and statistics\n    \"\"\"\n    print(\"=\"*60)\n    print(\"MODULAR NETWORK DATA ASSIGNMENT\")\n    print(\"=\"*60)\n    print(f\"AixLib version: {aixlib_version}\")\n    print(f\"Time index: {time_index}\")\n    print(f\"Graph: {len(graph.nodes)} nodes, {len(graph.edges)} edges\")\n    \n    # Step 1: Assign node values using flexible patterns\n    print(f\"\\n1. NODE VALUE ASSIGNMENT\")\n    print(\"-\" * 30)\n    \n    node_patterns = get_aixlib_masks_for_nodes(aixlib_version)\n    print(f\"Using patterns: {node_patterns}\")\n    \n    node_results = assign_node_values_flexible(\n        graph, df, port_mapping, node_patterns, time_index\n    )\n    \n    # Step 2: Assign edge values using AixLib masks\n    print(f\"\\n2. EDGE VALUE ASSIGNMENT\") \n    print(\"-\" * 30)\n    \n    edge_results = assign_edge_data_with_masks(\n        graph, df, aixlib_version, time_index\n    )\n    \n    # Step 3: Network statistics\n    print(f\"\\n3. NETWORK STATISTICS\")\n    print(\"-\" * 30)\n    \n    stats = calculate_network_statistics(graph)\n    \n    return {\n        \"node_results\": node_results,\n        \"edge_results\": edge_results,\n        \"network_stats\": stats,\n        \"config\": {\n            \"aixlib_version\": aixlib_version,\n            \"time_index\": time_index,\n            \"node_patterns\": node_patterns\n        }\n    }\n\n# Usage example with full modularity\nprint(\"=== MODULAR ASSIGNMENT EXAMPLE ===\")\nresults = full_network_assignment_modular(\n    graph=graph, \n    df=df, \n    port_mapping=port_mapping, \n    aixlib_version=\"2.1.0\", \n    time_index=0\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Example: Before/After demonstration of node value assignment\nprint(\"=== BEFORE NODE VALUE ASSIGNMENT ===\")\nsample_nodes = list(graph.nodes)[:5]  # Take first 5 nodes as examples\n\nprint(\"Sample nodes before assignment:\")\nfor node_id in sample_nodes:\n    node_data = graph.nodes[node_id]\n    temp = node_data.get('temperature', 'NOT SET')\n    pressure = node_data.get('pressure', 'NOT SET')\n    print(f\"Node {node_id}: Temperature = {temp}, Pressure = {pressure}\")\n\nprint(f\"\\nTotal nodes in graph: {len(graph.nodes)}\")\nnodes_with_temp = sum(1 for n in graph.nodes if 'temperature' in graph.nodes[n])\nnodes_with_pressure = sum(1 for n in graph.nodes if 'pressure' in graph.nodes[n])\nprint(f\"Nodes with temperature: {nodes_with_temp}\")\nprint(f\"Nodes with pressure: {nodes_with_pressure}\")\n\n# Run the assignment\nprint(\"\\n\" + \"=\"*50)\nprint(\"RUNNING NODE VALUE ASSIGNMENT...\")\nprint(\"=\"*50)\n\nconfig = {\n    \"temperature\": [\"sta_{port}.T\"],\n    \"pressure\": [\"port_{port}.p\"], \n}\n\nprocessed, inconsistencies, conflicts = assign_node_values_flexible(graph, df, port_mapping, config)\n\nprint(\"\\n=== AFTER NODE VALUE ASSIGNMENT ===\")\nprint(\"Sample nodes after assignment:\")\nfor node_id in sample_nodes:\n    node_data = graph.nodes[node_id]\n    temp = node_data.get('temperature', 'STILL NOT SET')\n    pressure = node_data.get('pressure', 'STILL NOT SET')\n    if isinstance(temp, float):\n        temp = f\"{temp:.3f}\"\n    if isinstance(pressure, float):\n        pressure = f\"{pressure:.3f}\"\n    print(f\"Node {node_id}: Temperature = {temp}, Pressure = {pressure}\")\n\n# Summary statistics\nnodes_with_temp_after = sum(1 for n in graph.nodes if 'temperature' in graph.nodes[n])\nnodes_with_pressure_after = sum(1 for n in graph.nodes if 'pressure' in graph.nodes[n])\nprint(f\"\\nSUMMARY:\")\nprint(f\"Nodes with temperature: {nodes_with_temp} → {nodes_with_temp_after}\")\nprint(f\"Nodes with pressure: {nodes_with_pressure} → {nodes_with_pressure_after}\")\nprint(f\"Success rate: {nodes_with_temp_after}/{len(graph.nodes)} = {nodes_with_temp_after/len(graph.nodes)*100:.1f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def assign_node_values_flexible(graph, df, port_mapping, attribute_config, time_index=0):\n    \"\"\"\n    Flexible node value assignment with pattern validation\n    \n    Now checks if multiple patterns for the same attribute give consistent results\n    \"\"\"\n    \n    processed_count = 0\n    inconsistency_count = 0\n    pattern_conflicts = 0\n    \n    for node_id, node_ports in port_mapping.items():\n        if not node_ports:\n            continue\n            \n        for attr_name, patterns in attribute_config.items():\n            all_values_by_pattern = {}  # Track values from each pattern\n            \n            for port in node_ports:\n                port_parts = port.split(\".\")\n                pipe_name = port_parts[0]\n                port_suffix = port_parts[1].split(\"_\")[-1]  # Extract 'a' or 'b'\n                \n                # Test ALL patterns and collect their values\n                for pattern in patterns:\n                    col_name = f\"networkModel.{pipe_name}.{pattern.format(port=port_suffix)}\"\n                    if col_name in df.columns:\n                        value = df[col_name].iloc[time_index]\n                        \n                        if pattern not in all_values_by_pattern:\n                            all_values_by_pattern[pattern] = []\n                        all_values_by_pattern[pattern].append(value)\n            \n            # Now check if different patterns give consistent results\n            if len(all_values_by_pattern) > 1:\n                # Multiple patterns found data - check consistency\n                pattern_means = {}\n                for pattern, values in all_values_by_pattern.items():\n                    pattern_means[pattern] = sum(values) / len(values)\n                \n                # Check if all patterns agree (within tolerance)\n                mean_values = list(pattern_means.values())\n                if len(set(round(v, 6) for v in mean_values)) > 1:\n                    print(f\"Pattern conflict for {attr_name} at node {node_id}:\")\n                    for pattern, mean_val in pattern_means.items():\n                        print(f\"  {pattern}: {mean_val:.6f}\")\n                    pattern_conflicts += 1\n            \n            # Use values from the first successful pattern\n            if all_values_by_pattern:\n                first_pattern = list(all_values_by_pattern.keys())[0]\n                collected_values = all_values_by_pattern[first_pattern]\n                \n                # Check consistency within the chosen pattern\n                unique_vals = set(round(v, 6) for v in collected_values)\n                \n                if len(unique_vals) == 1:\n                    graph.nodes[node_id][attr_name] = collected_values[0]\n                else:\n                    mean_val = sum(collected_values) / len(collected_values)\n                    graph.nodes[node_id][attr_name] = mean_val\n                    print(f\"Warning: Node {node_id} inconsistent {attr_name} within pattern: using mean {mean_val:.3f}\")\n                    inconsistency_count += 1\n        \n        processed_count += 1\n    \n    print(f\"Processed {processed_count} nodes\")\n    print(f\"Within-pattern inconsistencies: {inconsistency_count}\")\n    print(f\"Cross-pattern conflicts: {pattern_conflicts}\")\n    return processed_count, inconsistency_count, pattern_conflicts\n\n# Usage example with configurable patterns\nconfig = {\n    \"temperature\": [\"sta_{port}.T\"],\n    \"pressure\": [\"port_{port}.p\"], \n    # Future example: test multiple patterns for enthalpy\n    # \"enthalpy\": [\"port_{port}.h\", \"sta_{port}.h\"]  \n}\n\nprocessed, inconsistencies, conflicts = assign_node_values_flexible(graph, df, port_mapping, config)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uesgraphs1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}