{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import uesgraphs as ug\n",
    "\n",
    "from uesgraphs.examples import e1_example_readme as e1\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uesgraphs.analysis as analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "workspace = e1.workspace_example(\"e14\")\n",
    "\n",
    "dir_ues = os.path.dirname(os.path.dirname(workspace))\n",
    "pinola_json = os.path.join(dir_ues, \"workspace\", \"e11\", \"inputs\",\"test_modelgen\", \"Pinola\", \"nodes.json\")\n",
    "pinola_sim_data = os.path.join(dir_ues,\"uesgraphs\",\"data\",\"Pinola_low_temp_network_inputs.mat\")\n",
    "pinola_sim_data = r\"E:\\rka_lko\\work\\2025_04_analysis\\10042025SeestadtNewSim\\Sim20250409_190504_detailed\\Sim20250409_190504_1\\Results\\Sim20250409_190504_1_inputs.gzip\"\n",
    "pinola_json =r\"E:\\rka_lko\\git\\transurban_seestadt\\dhc_model\\workspace\\transurban_seestadt_uesgraphs.json\"\n",
    "sysm_model = r\"E:\\rka_lko\\git\\transurban_seestadt\\dhc_model\\workspace\\model.json\"\n",
    "if not os.path.exists(pinola_json):\n",
    "    raise FileNotFoundError(f\"File {pinola_json} not found.\"\n",
    "                            \"Please run example e11 to generate network topology.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read nodes...\n",
      "******\n",
      " input_ids were {'buildings': None, 'nodes': 'f607cb63-8aed-446c-9970-9cd8380434f8', 'pipes': None, 'supplies': None}\n",
      "...finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "graph = ug.UESGraph()\n",
    "graph.from_json(path = pinola_json, network_type=\"heating\")\n",
    "graph.graph[\"name\"] = \"pinola\"\n",
    "graph.graph[\"supply_type\"] = \"supply\"\n",
    "\n",
    "start_date=datetime(2024, 1, 1) \n",
    "end_date=datetime(2024, 1, 7)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gr = analysis.assign_simulation_data(graph, pinola_sim_data, start_date, end_date,aixlib_version=\"2.0.0\",auto_retry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logfile findable here: C:\\Users\\rka-lko\\AppData\\Local\\Temp\\3\\SystemModelHeating_20250828_184729.log\n",
      "Warning: Could not set attribute time: property 'time' of 'SystemModelHeating' object has no setter\n",
      "Model loaded from E:\\rka_lko\\git\\transurban_seestadt\\dhc_model\\workspace\\model.json\n"
     ]
    }
   ],
   "source": [
    "from uesgraphs.systemmodels import utilities as ut\n",
    "sysm_graph = ut.load_system_model_from_json(sysm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uesgraphs.systemmodels import utilities as ut\n",
    "sysm_graph = ut.load_system_model_from_json(sysm_model)\n",
    "from uesgraphs.analysis.data_handling import graph_transformation\n",
    "port_mapping = graph_transformation.map_system_model_to_uesgraph(sysm_graph,graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "uesgraphs.analysis.data_handling.data_handling.get_MASKS - INFO - Using AixLib version 2.1.0 masks\n",
      "uesgraphs.analysis.data_handling.data_handling.process_parquet_file - INFO - Found existing gzip file: E:\\rka_lko\\work\\2025_04_analysis\\10042025SeestadtNewSim\\Sim20250409_190504_detailed\\Sim20250409_190504_1\\Results\\Sim20250409_190504_1_inputs.gzip\n",
      "uesgraphs.analysis.data_handling.data_handling.process_parquet_file - INFO - Validating 72 required columns in: E:\\rka_lko\\work\\2025_04_analysis\\10042025SeestadtNewSim\\Sim20250409_190504_detailed\\Sim20250409_190504_1\\Results\\Sim20250409_190504_1_inputs.gzip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter-Liste mit 72 Variablen:\n",
      "  - networkModel.pipe10021010.port_a.m_flow\n",
      "  - networkModel.pipe10021010.dp\n",
      "  - networkModel.pipe10021010.port_a.p\n",
      "  - networkModel.pipe10021010.port_b.p\n",
      "  - networkModel.pipe10021010.sta_a.T\n",
      "  - networkModel.pipe10021010.sta_b.T\n",
      "  - networkModel.pipe10021014.port_a.m_flow\n",
      "  - networkModel.pipe10021014.dp\n",
      "  - networkModel.pipe10021014.port_a.p\n",
      "  - networkModel.pipe10021014.port_b.p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "uesgraphs.analysis.data_handling.data_handling.process_parquet_file - INFO - All required columns found in data file\n",
      "uesgraphs.analysis.data_handling.data_handling.process_parquet_file - INFO - Starting parquet file processing: E:\\rka_lko\\work\\2025_04_analysis\\10042025SeestadtNewSim\\Sim20250409_190504_detailed\\Sim20250409_190504_1\\Results\\Sim20250409_190504_1_inputs.gzip\n",
      "uesgraphs.analysis.data_handling.data_handling.process_parquet_file - INFO - Successfully loaded 35041 rows, 72 columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Shape: (35041, 72)\n",
      "Spalten: ['networkModel.pipe10021010.port_a.m_flow', 'networkModel.pipe10021010.dp', 'networkModel.pipe10021010.port_a.p', 'networkModel.pipe10021010.port_b.p', 'networkModel.pipe10021010.sta_a.T', 'networkModel.pipe10021010.sta_b.T', 'networkModel.pipe10021014.port_a.m_flow', 'networkModel.pipe10021014.dp', 'networkModel.pipe10021014.port_a.p', 'networkModel.pipe10021014.port_b.p', 'networkModel.pipe10021014.sta_a.T', 'networkModel.pipe10021014.sta_b.T', 'networkModel.pipe10021025.port_a.m_flow', 'networkModel.pipe10021025.dp', 'networkModel.pipe10021025.port_a.p', 'networkModel.pipe10021025.port_b.p', 'networkModel.pipe10021025.sta_a.T', 'networkModel.pipe10021025.sta_b.T', 'networkModel.pipe10041006.port_a.m_flow', 'networkModel.pipe10041006.dp', 'networkModel.pipe10041006.port_a.p', 'networkModel.pipe10041006.port_b.p', 'networkModel.pipe10041006.sta_a.T', 'networkModel.pipe10041006.sta_b.T', 'networkModel.pipe10041007.port_a.m_flow', 'networkModel.pipe10041007.dp', 'networkModel.pipe10041007.port_a.p', 'networkModel.pipe10041007.port_b.p', 'networkModel.pipe10041007.sta_a.T', 'networkModel.pipe10041007.sta_b.T', 'networkModel.pipe10041027.port_a.m_flow', 'networkModel.pipe10041027.dp', 'networkModel.pipe10041027.port_a.p', 'networkModel.pipe10041027.port_b.p', 'networkModel.pipe10041027.sta_a.T', 'networkModel.pipe10041027.sta_b.T', 'networkModel.pipe10061022.port_a.m_flow', 'networkModel.pipe10061022.dp', 'networkModel.pipe10061022.port_a.p', 'networkModel.pipe10061022.port_b.p', 'networkModel.pipe10061022.sta_a.T', 'networkModel.pipe10061022.sta_b.T', 'networkModel.pipe10071018.port_a.m_flow', 'networkModel.pipe10071018.dp', 'networkModel.pipe10071018.port_a.p', 'networkModel.pipe10071018.port_b.p', 'networkModel.pipe10071018.sta_a.T', 'networkModel.pipe10071018.sta_b.T', 'networkModel.pipe10101029.port_a.m_flow', 'networkModel.pipe10101029.dp', 'networkModel.pipe10101029.port_a.p', 'networkModel.pipe10101029.port_b.p', 'networkModel.pipe10101029.sta_a.T', 'networkModel.pipe10101029.sta_b.T', 'networkModel.pipe10141022.port_a.m_flow', 'networkModel.pipe10141022.dp', 'networkModel.pipe10141022.port_a.p', 'networkModel.pipe10141022.port_b.p', 'networkModel.pipe10141022.sta_a.T', 'networkModel.pipe10141022.sta_b.T', 'networkModel.pipe10141026.port_a.m_flow', 'networkModel.pipe10141026.dp', 'networkModel.pipe10141026.port_a.p', 'networkModel.pipe10141026.port_b.p', 'networkModel.pipe10141026.sta_a.T', 'networkModel.pipe10141026.sta_b.T', 'networkModel.pipe10181028.port_a.m_flow', 'networkModel.pipe10181028.dp', 'networkModel.pipe10181028.port_a.p', 'networkModel.pipe10181028.port_b.p', 'networkModel.pipe10181028.sta_a.T', 'networkModel.pipe10181028.sta_b.T']\n"
     ]
    }
   ],
   "source": [
    "# 1. Supply Type und Masks laden\n",
    "supply_type = graph.graph[\"supply_type\"]  # \"supply\" oder \"return\"\n",
    "supply_type_prefix = {\"supply\": \"\", \"return\": \"R\"}\n",
    "\n",
    "from uesgraphs.analysis.data_handling.data_handling import get_MASKS\n",
    "masks = get_MASKS(\"2.1.0\")  # oder deine gew√ºnschte Version\n",
    "\n",
    "# 2. Filter-Liste bauen (wie im existierenden Code)\n",
    "filter_list = []\n",
    "for edge in graph.edges:\n",
    "    pipe_code = graph.edges[edge][\"name\"]\n",
    "    for mask_type, mask_pattern in masks.items():\n",
    "        var_name = mask_pattern.format(\n",
    "            pipe_code=pipe_code, \n",
    "            type=supply_type_prefix[supply_type]\n",
    "        )\n",
    "        filter_list.append(var_name)\n",
    "\n",
    "print(f\"Filter-Liste mit {len(filter_list)} Variablen:\")\n",
    "for var in filter_list[:10]:  # Erste 10 zeigen\n",
    "    print(f\"  - {var}\")\n",
    "\n",
    "# 3. Daten laden\n",
    "from uesgraphs.analysis.data_handling.data_handling import process_simulation_result\n",
    "df = process_simulation_result(file_path=pinola_sim_data, filter_list=filter_list)\n",
    "\n",
    "print(f\"\\nDataFrame Shape: {df.shape}\")\n",
    "print(f\"Spalten: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1001\n",
    "node_ports = port_mapping[i]  # Liste aller Ports f√ºr diesen Node\n",
    "collected_values = {\"T\": [], \"p\": []}\n",
    "\n",
    "# Mapping f√ºr flexible Attributnamen im Graph\n",
    "attribute_names = {\"T\": \"temperature\", \"p\": \"press_flow\"}  # Konfigurierbar!\n",
    "\n",
    "for port in node_ports:  # Iteriere √ºber ALLE Ports des Nodes\n",
    "    port_splitted = port.split(\".\")\n",
    "    if port_splitted[1].endswith(\"a\"):\n",
    "        #port is an a port\n",
    "        temp = f\"networkModel.{port_splitted[0]}.sta_a.T\"\n",
    "        press = f\"networkModel.{port_splitted[0]}.port_a.p\"\n",
    "        t_value = df[temp].iloc[0]\n",
    "        p_value = df[press].iloc[0]\n",
    "        collected_values[\"T\"].append(t_value)\n",
    "        collected_values[\"p\"].append(p_value)\n",
    "\n",
    "# Validiere und weise zu\n",
    "for size in [\"T\", \"p\"]:\n",
    "    unique_vals = set(collected_values[size])\n",
    "    if len(unique_vals) != 1:\n",
    "        raise ValueError(f\"Node {i}: Inkonsistente {size}-Werte gefunden: {collected_values[size]}\")\n",
    "    graph.nodes[i][attribute_names[size]] = collected_values[size][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Port Mapping f√ºr Knoten 1001: pipe10021025.port_a\n",
      "Gefundene Spalten: ['networkModel.pipe10021014.port_a.m_flow', 'networkModel.pipe10021014.dp', 'networkModel.pipe10021014.port_a.p', 'networkModel.pipe10021014.port_b.p', 'networkModel.pipe10021014.sta_a.T', 'networkModel.pipe10021014.sta_b.T']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Port Mapping f√ºr Knoten {i}: {port}\")\n",
    "pipe_mask = df.columns.str.contains('pipe10021014.')\n",
    "df_pipe = df.loc[:, pipe_mask]\n",
    "print(f\"Gefundene Spalten: {df.columns[pipe_mask].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge (1001, 1005) has data: {'diameter': 0.0697, 'length': 62.67448664094058, 'pipeID': '10021010', 'name': '10021010', 'node_0': 1002, 'node_1': 1010}\n",
      "Edge (1001, 1006) has data: {'diameter': 0.2101, 'length': 40.10060652984215, 'pipeID': '10021014', 'name': '10021014', 'node_0': 1002, 'node_1': 1014}\n",
      "Edge (1001, 1009) has data: {'diameter': 0.2101, 'length': 23.793988913832845, 'pipeID': '10021025', 'name': '10021025', 'node_0': 1002, 'node_1': 'supply1'}\n",
      "Edge (1002, 1003) has data: {'diameter': 0.1325, 'length': 23.15227945754485, 'pipeID': '10041006', 'name': '10041006', 'node_0': 1004, 'node_1': 1006}\n",
      "Edge (1002, 1004) has data: {'diameter': 0.1325, 'length': 67.21343661070965, 'pipeID': '10041007', 'name': '10041007', 'node_0': 1004, 'node_1': 1007}\n",
      "Edge (1002, 1011) has data: {'diameter': 0.0539, 'length': 25.20026053450003, 'pipeID': '10041027', 'name': '10041027', 'node_0': 1004, 'node_1': 'w11_2'}\n",
      "Edge (1003, 1008) has data: {'diameter': 0.2101, 'length': 43.10762795752977, 'pipeID': '10061022', 'name': '10061022', 'node_0': 1006, 'node_1': 1022}\n",
      "Edge (1004, 1007) has data: {'diameter': 0.1071, 'length': 41.06543024277208, 'pipeID': '10071018', 'name': '10071018', 'node_0': 1007, 'node_1': 1018}\n",
      "Edge (1005, 1013) has data: {'diameter': 0.0697, 'length': 17.635612191884555, 'pipeID': '10101029', 'name': '10101029', 'node_0': 1010, 'node_1': 'w11_4'}\n",
      "Edge (1006, 1008) has data: {'diameter': 0.2101, 'length': 42.097964605664814, 'pipeID': '10141022', 'name': '10141022', 'node_0': 1014, 'node_1': 1022}\n",
      "Edge (1006, 1010) has data: {'diameter': 0.0539, 'length': 18.20446528640188, 'pipeID': '10141026', 'name': '10141026', 'node_0': 1014, 'node_1': 'w11_3'}\n",
      "Edge (1007, 1012) has data: {'diameter': 0.0697, 'length': 22.026936306526547, 'pipeID': '10181028', 'name': '10181028', 'node_0': 1018, 'node_1': 'w11_1'}\n"
     ]
    }
   ],
   "source": [
    "edges = list(graph.edges)\n",
    "for edge in edges:\n",
    "    print(f\"Edge {edge} has data: {graph.edges[edge]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2453415739.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mgraph = analyze.assign_data_to_uesgraphs(graph,sim_data = pinola_sim_data,\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "    graph = analyze.assign_data_to_uesgraphs(graph,sim_data = pinola_sim_data,\n",
    "                                             start_date=start_date,\n",
    "                                             end_date=end_date,\n",
    "                                             aixlib_version=\"2.1.0\") #aixlib version is needed to assign data properly\n",
    "    \n",
    "    ### Plotting\n",
    "    #Visuals cant handle series data, so we just take the mean values, but single time points are possible\n",
    "    vis = ug.Visuals(graph)\n",
    "    for edge in graph.edges:\n",
    "        graph.edges[edge][\"m_flow_mean\"] = graph.edges[edge][\"m_flow\"].mean()\n",
    "    vis.show_network(show_plot=False,\n",
    "                           scaling_factor=1,\n",
    "                           scaling_factor_diameter=50,\n",
    "                           label_size=15,\n",
    "                           ylabel=\"Mean mass flow [kg/s]\",\n",
    "                           generic_extensive_size=\"m_flow_mean\",\n",
    "                           save_as=os.path.join(workspace, \"m_flow.png\"),\n",
    "                           timestamp=f\"{graph.graph[\"name\"]}: Mean mass flow\"\n",
    "                           )\n",
    "    \n",
    "    for node in graph.nodes:\n",
    "        graph.nodes[node][\"press_flow_mean\"] = graph.nodes[node][\"press_flow\"].mean()\n",
    "    vis.show_network(show_plot=False,\n",
    "                           scaling_factor=1,\n",
    "                           scaling_factor_diameter=50,\n",
    "                           ylabel=\"Mean pressure [Pa]\",\n",
    "                           label_size=15,\n",
    "                           generic_intensive_size=\"press_flow_mean\",\n",
    "                           save_as=os.path.join(workspace, \"press_flow.png\"),\n",
    "                           timestamp=f\"{graph.graph[\"name\"]}: Mean pressure flow\"\n",
    "                           )\n",
    "    \n",
    "    df = analyze.pump_power_analysis(graph, True, r\"D:\\rka-lko\\work\\2025_04_analysis\\3\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIXLIB_MASKS = {\n",
    "    \"2.1.0\": {\n",
    "        \"edge\": {\n",
    "            # Extensive properties - same value at both ports\n",
    "            \"m_flow\": \"networkModel.pipe{pipe_code}{type}.port_a.m_flow\",\n",
    "            \"dp\": \"networkModel.pipe{pipe_code}{type}.dp\",\n",
    "        },\n",
    "        \"node\": {\n",
    "            # Intensive properties - may differ between ports\n",
    "            \"pressure\": {\n",
    "                \"port_a\": \"networkModel.pipe{pipe_code}{type}.port_a.p\",\n",
    "                \"port_b\": \"networkModel.pipe{pipe_code}{type}.port_b.p\"\n",
    "            },\n",
    "            \"temperature\": {\n",
    "                \"port_a\": \"networkModel.pipe{pipe_code}{type}.sta_a.T\", \n",
    "                \"port_b\": \"networkModel.pipe{pipe_code}{type}.sta_b.T\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"2.0.0\": {\n",
    "        \"edge\": {\n",
    "            # Extensive properties - same value at both ports\n",
    "            \"m_flow\": \"networkModel.pipe{pipe_code}{type}.port_a.m_flow\",\n",
    "        },\n",
    "        \"node\": {\n",
    "            # Intensive properties - may differ between ports\n",
    "            \"pressure\": {\n",
    "                \"port_a\": \"networkModel.pipe{pipe_code}{type}.port_a.p\",\n",
    "                \"port_b\": \"networkModel.pipe{pipe_code}{type}.ports_b[1].p\"\n",
    "            },\n",
    "            \"temperature\": {\n",
    "                \"port_a\": \"networkModel.pipe{pipe_code}{type}.sta_a.T\",\n",
    "                \"port_b\": \"networkModel.pipe{pipe_code}{type}.sta_b[1].T\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_supply_type_prefix(graph):\n",
    "    supply_type = graph.graph.get(\"supply_type\", \"supply\")\n",
    "    supply_type_prefix = {\"supply\": \"\", \"return\": \"R\"}\n",
    "    return supply_type_prefix.get(supply_type, \"\")\n",
    "\n",
    "def build_filter_list_pipe(graph,MASK, logger=None):\n",
    "    if logger is None:\n",
    "        import logging\n",
    "        logger = logging.getLogger(__name__)\n",
    "    mask_sim = []\n",
    "    for category_name, category in MASK.items():\n",
    "        if category_name in [\"edge\", \"node\"]:\n",
    "            if category_name == \"edge\":\n",
    "                # Edge values: direct access to patterns\n",
    "                mask_sim.extend(category.values())\n",
    "            else:  # node\n",
    "                # Node values: nested structure - extract all port patterns\n",
    "                for attribute_patterns in category.values():\n",
    "                    mask_sim.extend(attribute_patterns.values())\n",
    "\n",
    "    type_prefix = get_supply_type_prefix(graph)\n",
    "\n",
    "    filter_list = []\n",
    "    for edge in graph.edges:\n",
    "        pipe_code = graph.edges[edge][\"name\"]\n",
    "        for entry_sim in mask_sim:\n",
    "            var_name = entry_sim.format(\n",
    "                pipe_code=pipe_code, \n",
    "                type=type_prefix\n",
    "            )\n",
    "            filter_list.append(var_name)\n",
    "    return filter_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def assign_node_values(graph, df, port_mapping, mask, time_index=0, logger=None):\n",
    "    \"\"\"\n",
    "    Assigns node values from simulation data using flexible mask configuration.\n",
    "    \n",
    "    Processes intensive properties (pressure, temperature) that may differ between \n",
    "    ports of the same pipe, unlike extensive properties (mass flow) that are \n",
    "    identical at both ports.\n",
    "    \n",
    "    Args:\n",
    "        graph: NetworkX graph with nodes to assign values to\n",
    "        df: DataFrame containing simulation data\n",
    "        port_mapping: Dict mapping node_ids to list of connected ports.\n",
    "                     Example: {1: ['pipe001.port_a', 'pipe002.port_b'], \n",
    "                              2: ['pipe001.port_b', 'pipe003.port_a']}\n",
    "                     Source: [Method name if known, otherwise leave empty]\n",
    "        mask: Mask dictionary containing node configuration for intensive properties\n",
    "        time_index: Time step index to extract from df (default: 0)\n",
    "        logger: Logger instance (optional, creates terminal logger if None)\n",
    "    \"\"\"\n",
    "    if logger is None:\n",
    "        logger = set_up_terminal_logger(f\"{__name__}.assign_node_values\")\n",
    "    \n",
    "    # Extract node configuration\n",
    "    node_config = mask.get(\"node\", {})\n",
    "    if not node_config:\n",
    "        logger.error(\"No 'node' configuration found in mask\")\n",
    "        return\n",
    "    \n",
    "    type_suffix = get_supply_type_prefix(graph)\n",
    "    \n",
    "    stats =  {\n",
    "        'processed_count': 0,\n",
    "        'inconsistency_count': 0, \n",
    "        'pattern_conflicts': 0\n",
    "    }\n",
    "    \n",
    "    for node_id, node_ports in port_mapping.items():\n",
    "        if not node_ports:\n",
    "            continue\n",
    "            \n",
    "        _assign_attributes_to_node(\n",
    "            graph, node_id, node_ports, df, node_config,\n",
    "            type_suffix, stats, logger\n",
    "        )\n",
    "        \n",
    "        stats['processed_count'] += 1\n",
    "    \n",
    "    logger.info(f\"Assignment completed:\")\n",
    "    logger.info(f\"  Processed nodes: {stats['processed_count']}\")\n",
    "    logger.info(f\"  Within-pattern inconsistencies: {stats['inconsistency_count']}\")\n",
    "    logger.info(f\"  Cross-pattern conflicts: {stats['pattern_conflicts']}\")\n",
    "\n",
    "def _assign_attributes_to_node(graph, node_id, node_ports, df, config, \n",
    "                              type_suffix, stats, logger):\n",
    "    \"\"\"Assign all attributes to a single node.\n",
    "        config: {\"attribute\": {\"port_suffix\": \"pattern_with_{pipe_code}\"}}\n",
    "                ex.: {\"temperature\": {\"port_a\": \"networkModel.pipe{pipe_code}{type}.sta_a.T\"}}\n",
    "    \"\"\"\n",
    "    \n",
    "    for attribute_name, port_patterns in config.items():\n",
    "        \"\"\"Collect values for a specific attribute from all relevant ports.\n",
    "        attribute_name: e.g. \"temperature\"\n",
    "        port_patterns: e.g. {\"port_a\": \"networkModel.pipe{pipe_code}{type}.sta_a.T\",\n",
    "                        \"port_b\": \"networkModel.pipe{pipe_code}{type}.sta_b.T\"}\n",
    "        \"\"\"\n",
    "        series_list = []\n",
    "        for port in node_ports:\n",
    "            \n",
    "            pipe_name, port_suffix = _parse_port_identifier(port)\n",
    "            if port_suffix in port_patterns:\n",
    "                pattern = port_patterns[port_suffix]\n",
    "                column_name = pattern.format(pipe_code=pipe_name, type=type_suffix)\n",
    "            \n",
    "                if column_name in df.columns:\n",
    "                    series = df[column_name]\n",
    "                    series_list.append(series)\n",
    "                else:\n",
    "                    logger.debug(f\"Column not found: {column_name}\")\n",
    "\n",
    "        if len(series_list) == 0:\n",
    "            logger.debug(f\"No data found for {attribute_name} at node {node_id}\")\n",
    "            continue\n",
    "\n",
    "        # Check if all series are identical\n",
    "        if len(series_list) > 1:\n",
    "            all_equal = all(series_list[0].equals(series) for series in series_list[1:])\n",
    "            if not all_equal:\n",
    "                logger.warning(f\"Node {node_id}: Inconsistent {attribute_name} time series found\")\n",
    "                stats['inconsistency_count'] += 1\n",
    "        \n",
    "        # Use first series as result\n",
    "        graph.nodes[node_id][attribute_name] = series_list[0]        \n",
    "\n",
    "def _parse_port_identifier(port):\n",
    "    \"\"\"Parse port identifier to extract pipe name and port suffix.\"\"\"\n",
    "    port_parts = port.split(\".\")\n",
    "    if len(port_parts) < 2:\n",
    "        raise ValueError(f\"Invalid port format: {port}. Expected 'pipe_name.port_suffix'\")\n",
    "    return port_parts[0].replace(\"pipe\", \"\"), port_parts[1]\n",
    "\n",
    "\n",
    "## Edges\n",
    "def assign_edge_data(graph, MASK, df):\n",
    "    type_suffix = get_supply_type_prefix(graph)\n",
    "    for edge in graph.edges:\n",
    "        for edge_variable, variable_mask in MASK[\"edge\"].items():\n",
    "            pipe_name = graph.edges[edge][\"name\"]\n",
    "            variable_name = variable_mask.format(pipe_code=pipe_name, type=type_suffix)\n",
    "            graph.edges[edge][edge_variable] = df[variable_name]\n",
    "        \n",
    "## validation\n",
    "def validate_graph_attributes(graph, mask, reference_df, logger=None):\n",
    "    \"\"\"\n",
    "    Validates graph edge and node attributes against a reference DataFrame.\n",
    "    \n",
    "    Checks if the length of attribute arrays in edges and nodes matches the \n",
    "    number of rows in the reference DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        graph: NetworkX Graph object with edges and nodes\n",
    "        mask (dict): Dictionary with \"edge\" and \"node\" keys containing \n",
    "                    attributes to validate\n",
    "        reference_df (pd.DataFrame): Reference DataFrame for length comparison\n",
    "        logger (logging.Logger, optional): Logger instance. If None, a \n",
    "                                         terminal logger will be created.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if all validations pass, False otherwise\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: For critical validation errors\n",
    "    \"\"\"\n",
    "    if logger is None:\n",
    "        logger = set_up_terminal_logger(f\"{__name__}.validate_graph_attributes\")\n",
    "    \n",
    "    logger.info(\"Starting graph attribute validation...\")\n",
    "    \n",
    "    expected_length = reference_df.shape[0]\n",
    "    validation_passed = True\n",
    "    errors = []\n",
    "    \n",
    "    # Validate edge attributes\n",
    "    logger.info(f\"Validating edge attributes for {len(graph.edges)} edges...\")\n",
    "    \n",
    "    if \"edge\" in mask:\n",
    "        for edge_idx, edge in enumerate(graph.edges):\n",
    "            for edge_attr in mask[\"edge\"].keys():\n",
    "                if edge_attr in graph.edges[edge]:\n",
    "                    actual_length = len(graph.edges[edge][edge_attr])\n",
    "                    \n",
    "                    if actual_length != expected_length:\n",
    "                        error_msg = (\n",
    "                            f\"Edge {edge} - Attribute '{edge_attr}': \"\n",
    "                            f\"length {actual_length} != expected length {expected_length}\"\n",
    "                        )\n",
    "                        logger.error(error_msg)\n",
    "                        errors.append(error_msg)\n",
    "                        validation_passed = False\n",
    "                    else:\n",
    "                        logger.debug(\n",
    "                            f\"Edge {edge} - Attribute '{edge_attr}': OK \"\n",
    "                            f\"(length: {actual_length})\"\n",
    "                        )\n",
    "                else:\n",
    "                    warning_msg = f\"Edge {edge} - Attribute '{edge_attr}' not found\"\n",
    "                    logger.warning(warning_msg)\n",
    "    else:\n",
    "        logger.warning(\"No edge attributes defined in MASK\")\n",
    "    \n",
    "    # Validate node attributes\n",
    "    logger.info(f\"Validating node attributes for {len(graph.nodes)} nodes...\")\n",
    "    \n",
    "    if \"node\" in mask:\n",
    "        for node_idx, node in enumerate(graph.nodes):\n",
    "            for node_attr in mask[\"node\"].keys():\n",
    "                if node_attr in graph.nodes[node]:\n",
    "                    actual_length = len(graph.nodes[node][node_attr])\n",
    "                    \n",
    "                    if actual_length != expected_length:\n",
    "                        error_msg = (\n",
    "                            f\"Node {node} - Attribute '{node_attr}': \"\n",
    "                            f\"length {actual_length} != expected length {expected_length}\"\n",
    "                        )\n",
    "                        logger.error(error_msg)\n",
    "                        errors.append(error_msg)\n",
    "                        validation_passed = False\n",
    "                    else:\n",
    "                        logger.debug(\n",
    "                            f\"Node {node} - Attribute '{node_attr}': OK \"\n",
    "                            f\"(length: {actual_length})\"\n",
    "                        )\n",
    "                else:\n",
    "                    warning_msg = f\"Node {node} - Attribute '{node_attr}' not found\"\n",
    "                    logger.warning(warning_msg)\n",
    "    else:\n",
    "        logger.warning(\"No node attributes defined in MASK\")\n",
    "    \n",
    "    # Validation summary\n",
    "    if validation_passed:\n",
    "        logger.info(\"‚úÖ Graph attribute validation completed successfully\")\n",
    "    else:\n",
    "        logger.error(f\"‚ùå Graph attribute validation failed: {len(errors)} errors\")\n",
    "        for error in errors[:5]:  # Show maximum 5 errors in summary\n",
    "            logger.error(f\"  - {error}\")\n",
    "        if len(errors) > 5:\n",
    "            logger.error(f\"  ... and {len(errors) - 5} more errors\")\n",
    "    \n",
    "    return validation_passed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logfile findable here: C:\\Users\\rka-lko\\AppData\\Local\\Temp\\3\\all_20250828_185501.log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uesgraphs.analysis.data_handling.data_handling import validate_columns_exist, check_supply_type, set_up_file_logger, prepare_DataFrame, set_up_terminal_logger\n",
    "aixlib_version = \"2.1.0\"\n",
    "time_interval = \"15min\"\n",
    "\n",
    "from uesgraphs.systemmodels import utilities as ut\n",
    "sysm_graph = ut.load_system_model_from_json(sysm_model)\n",
    "from uesgraphs.analysis.data_handling import graph_transformation\n",
    "port_mapping = graph_transformation.map_system_model_to_uesgraph(sysm_graph,graph)\n",
    "\n",
    "logger = set_up_file_logger(\"all\")\n",
    "MASK = AIXLIB_MASKS[aixlib_version]\n",
    "filter_list = build_filter_list_pipe(graph,MASK=MASK,logger=logger)\n",
    "m = validate_columns_exist(file_path=pinola_sim_data, required_columns=filter_list,logger=logger)\n",
    "\n",
    "df = process_simulation_result(file_path=pinola_sim_data, filter_list=filter_list, logger=logger)\n",
    "df = prepare_DataFrame(df, start_date=start_date, end_date=end_date, \n",
    "                             time_interval=time_interval, logger=logger)\n",
    "\n",
    "assign_node_values(graph, df, port_mapping, MASK, logger=logger)\n",
    "assign_edge_data(graph, MASK, df)\n",
    "\n",
    "validate_graph_attributes(graph, MASK, df,logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__.validate_graph_attributes - INFO - Starte Graph-Attribut-Validierung...\n",
      "__main__.validate_graph_attributes - INFO - Validiere Edge-Attribute f√ºr 12 Edges...\n",
      "__main__.validate_graph_attributes - INFO - Validiere Node-Attribute f√ºr 13 Nodes...\n",
      "__main__.validate_graph_attributes - INFO - ‚úÖ Graph-Attribut-Validierung erfolgreich abgeschlossen\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uesgraphs1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
